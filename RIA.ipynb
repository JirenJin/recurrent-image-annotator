{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define VGG-16 Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from kaffe.tensorflow import Network\n",
    "\n",
    "class VGG_ILSVRC_16_layers(Network):\n",
    "    def setup(self):\n",
    "        (self.feed('input')\n",
    "             .conv(3, 3, 64, 1, 1, name='conv1_1')\n",
    "             .conv(3, 3, 64, 1, 1, name='conv1_2')\n",
    "             .max_pool(2, 2, 2, 2, name='pool1')\n",
    "             .conv(3, 3, 128, 1, 1, name='conv2_1')\n",
    "             .conv(3, 3, 128, 1, 1, name='conv2_2')\n",
    "             .max_pool(2, 2, 2, 2, name='pool2')\n",
    "             .conv(3, 3, 256, 1, 1, name='conv3_1')\n",
    "             .conv(3, 3, 256, 1, 1, name='conv3_2')\n",
    "             .conv(3, 3, 256, 1, 1, name='conv3_3')\n",
    "             .max_pool(2, 2, 2, 2, name='pool3')\n",
    "             .conv(3, 3, 512, 1, 1, name='conv4_1')\n",
    "             .conv(3, 3, 512, 1, 1, name='conv4_2')\n",
    "             .conv(3, 3, 512, 1, 1, name='conv4_3')\n",
    "             .max_pool(2, 2, 2, 2, name='pool4')\n",
    "             .conv(3, 3, 512, 1, 1, name='conv5_1')\n",
    "             .conv(3, 3, 512, 1, 1, name='conv5_2')\n",
    "             .conv(3, 3, 512, 1, 1, name='conv5_3')\n",
    "             .max_pool(2, 2, 2, 2, name='pool5')\n",
    "             .fc(4096, name='fc6')\n",
    "             .fc(4096, name='fc7')\n",
    "             .fc(1000, relu=False, name='fc8')\n",
    "             .softmax(name='prob'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "# initialize VGG-16 net\n",
    "net = VGG_ILSVRC_16_layers({'input': inputs})\n",
    "# we only need to extract fc7 layer features\n",
    "fc7 = net.layers['fc7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_images(image_list):\n",
    "    \"\"\"Load images into numpy arrays from an image_list.\"\"\"\n",
    "    # specific for vgg model, subtract mean pixel value\n",
    "    mean_pixel = [103.939, 116.779, 123.68]\n",
    "    image_full_batch = []\n",
    "    for image_path in image_list:\n",
    "        # 1 means read color image\n",
    "        image_tensor = cv2.imread(image_path, 1).astype('float32')\n",
    "        image_full_batch.append(image_tensor)\n",
    "    # put all image tensors together into the full batch\n",
    "    image_full_batch = np.stack(image_full_batch)\n",
    "    for c in xrange(3):\n",
    "        image_full_batch[:, :, :, c] -= mean_pixel[c]\n",
    "    return image_full_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(inputs, batch_size=100):\n",
    "    \"\"\"Generate mini batches for given inputs.\"\"\"\n",
    "    full_size = inputs.shape[0]\n",
    "    iterations = full_size // batch_size\n",
    "    count = 0\n",
    "    while count < iterations:\n",
    "        yield inputs[count*batch_size:(count+1)*batch_size]\n",
    "        count +=1\n",
    "    # finally, yield data that has not been used yet \n",
    "    # maybe we should check the size, in case that there is only one example left (dims decreases)\n",
    "    if count*batch_size != full_size:\n",
    "        yield inputs[count*batch_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_image_features(images):\n",
    "    \"\"\"Extract fc7 features using vgg-16 model.\"\"\"\n",
    "    with tf.Session() as sess:\n",
    "        # Load the data\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        net.load('vgg_16.tfmodel', sess)\n",
    "\n",
    "        # train_image_embeddings\n",
    "        image_features = []\n",
    "        for image_batch in generate_batch(images, batch_size = 200):\n",
    "            feed = {inputs: image_batch}\n",
    "            batch_image_features = sess.run([fc7], feed_dict=feed)\n",
    "            # notice the 0, since the dimension of bacth_image_features is 1xbatch_sizex4096\n",
    "            image_features.append(batch_image_features[0])\n",
    "            \n",
    "    image_features = np.vstack(image_features)\n",
    "    return image_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Corel5k/train_image_list.json') as f:\n",
    "    train_image_list = json.load(f)\n",
    "train_images = load_images(train_image_list)\n",
    "\n",
    "with open('Corel5k/test_image_list.json') as f:\n",
    "    test_image_list = json.load(f)\n",
    "test_images = load_images(test_image_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract image features and save for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_image_features = extract_image_features(train_images)\n",
    "#with open(\"train_image_features.npy\", \"w\") as f:\n",
    "    #np.save(f, train_image_features)\n",
    "\n",
    "test_image_features = extract_image_features(test_images)\n",
    "#with open(\"test_image_features.npy\", \"w\") as f:\n",
    "    #np.save(f, test_image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load image features directly\n",
    "with open(\"train_image_features.npy\", \"r\") as f:\n",
    "    train_image_features = np.load(f)\n",
    "    \n",
    "with open(\"test_image_features.npy\", \"r\") as f:\n",
    "    test_image_features = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_hidden = 512\n",
    "\n",
    "image_features = tf.placeholder(tf.float32, [None, 4096])\n",
    "image_embedding_weights = tf.Variable(tf.truncated_normal([4096, num_hidden], stddev=0.01))\n",
    "image_embedding_bias = tf.Variable(tf.constant(0.1, shape=[num_hidden]))\n",
    "h0 = tf.matmul(image_features, image_embedding_weights) + image_embedding_bias\n",
    "c0 = tf.zeros_like(h0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load annotations and create label inputs/targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260\n"
     ]
    }
   ],
   "source": [
    "with open('Corel5k/train_annotations.json') as f:\n",
    "    train_annotations = json.load(f)\n",
    "\n",
    "with open('Corel5k/test_annotations.json') as f:\n",
    "    test_annotations = json.load(f)\n",
    "\n",
    "vocabulary_size = max([max(x) for x in train_annotations if x]) # for Corel5k specifically   \n",
    "print vocabulary_size\n",
    "\n",
    "# create inputs and targets with \"START\"/\"STOP\" signal\n",
    "# note that Python is 0-indexed, and labels do not use 0, so we use 0 for special signal\n",
    "train_label_inputs = [[0] + annotation for annotation in train_annotations]\n",
    "train_label_targets = [annotation + [0] for annotation in train_annotations]\n",
    "# for test data\n",
    "test_label_inputs = [[0] + annotation for annotation in test_annotations]\n",
    "test_label_targets = [annotation + [0] for annotation in test_annotations]\n",
    "\n",
    "# transform to one-hot vector\n",
    "train_label_targets = [np.eye(vocabulary_size+1)[x] for x in train_label_targets]\n",
    "test_label_targets = [np.eye(vocabulary_size+1)[x] for x in test_label_targets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_size = 512\n",
    "max_length = 6 # for Corel5k, maximum length of annotation is 5\n",
    "\n",
    "# note that we have another START or STOP signal in addition to the real labels\n",
    "label_embedding_matrix = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size + 1, embedding_size], -1.0, 1.0))\n",
    "\n",
    "label_input = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# Padding label inputs with zero-vectors\n",
    "label_embedding = tf.concat(\n",
    "    0, [tf.nn.embedding_lookup(label_embedding_matrix, label_input), \n",
    "        tf.zeros([max_length - tf.shape(label_input)[0], embedding_size])])\n",
    "# add one new axis (batch_size==1)\n",
    "label_embedding = tf.reshape(label_embedding, [1,-1,embedding_size])\n",
    "\n",
    "# mask for setting sequence_length for each sample\n",
    "def length(data):\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(data), reduction_indices=2))\n",
    "    length = tf.reduce_sum(used, reduction_indices=1)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "num_layers = 1\n",
    "dropout = tf.placeholder(tf.float32)\n",
    "\n",
    "# Why use state_is_tuple???\n",
    "lstm = tf.nn.rnn_cell.LSTMCell(num_hidden, state_is_tuple=True)  \n",
    "lstm = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=dropout)\n",
    "lstm = tf.nn.rnn_cell.MultiRNNCell([lstm] * num_layers, state_is_tuple=True)\n",
    "output, state = tf.nn.dynamic_rnn(lstm, label_embedding, dtype=tf.float32, \n",
    "                                  initial_state=(c0, h0), sequence_length=length(label_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Softmax prediction and compute cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reshape output to be used in mini-batch fc layer\n",
    "output = tf.reshape(output, [-1, num_hidden])\n",
    "output = tf.gather(output, tf.range(length(label_embedding)[0]))\n",
    "# fc layer\n",
    "weight = tf.Variable(tf.truncated_normal([num_hidden, vocabulary_size+1], stddev=0.01))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[vocabulary_size+1]))\n",
    "\n",
    "prediction = tf.nn.softmax(tf.matmul(output, weight) + bias)\n",
    "\n",
    "# target should be one-hot vector?\n",
    "target = tf.placeholder(tf.float32, [None, vocabulary_size + 1])\n",
    "cross_entropy = -tf.reduce_sum(\n",
    "    target * tf.log(prediction), reduction_indices=[1])\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.0001, epsilon=1e-2).minimize(cross_entropy)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(target, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iteration: 200, batch_loss: 5.511, batch_accuracy: 0.194\n",
      "epoch: 0, iteration: 400, batch_loss: 5.325, batch_accuracy: 0.236\n",
      "epoch: 0, iteration: 600, batch_loss: 4.898, batch_accuracy: 0.270\n",
      "epoch: 0, iteration: 800, batch_loss: 4.206, batch_accuracy: 0.242\n",
      "epoch: 0, iteration: 1000, batch_loss: 3.985, batch_accuracy: 0.254\n",
      "epoch: 0, iteration: 1200, batch_loss: 4.100, batch_accuracy: 0.272\n",
      "epoch: 0, iteration: 1400, batch_loss: 3.675, batch_accuracy: 0.326\n",
      "epoch: 0, iteration: 1600, batch_loss: 4.122, batch_accuracy: 0.288\n",
      "epoch: 0, iteration: 1800, batch_loss: 3.866, batch_accuracy: 0.247\n",
      "epoch: 0, iteration: 2000, batch_loss: 3.929, batch_accuracy: 0.268\n",
      "epoch: 0, iteration: 2200, batch_loss: 3.768, batch_accuracy: 0.300\n",
      "epoch: 0, iteration: 2400, batch_loss: 3.753, batch_accuracy: 0.301\n",
      "epoch: 0, iteration: 2600, batch_loss: 3.533, batch_accuracy: 0.268\n",
      "epoch: 0, iteration: 2800, batch_loss: 3.560, batch_accuracy: 0.292\n",
      "epoch: 0, iteration: 3000, batch_loss: 3.558, batch_accuracy: 0.294\n",
      "epoch: 0, iteration: 3200, batch_loss: 3.402, batch_accuracy: 0.349\n",
      "epoch: 0, iteration: 3400, batch_loss: 3.743, batch_accuracy: 0.278\n",
      "epoch: 0, iteration: 3600, batch_loss: 3.544, batch_accuracy: 0.349\n",
      "epoch: 0, iteration: 3800, batch_loss: 3.785, batch_accuracy: 0.251\n",
      "epoch: 0, iteration: 4000, batch_loss: 3.440, batch_accuracy: 0.375\n",
      "epoch: 0, iteration: 4200, batch_loss: 3.289, batch_accuracy: 0.315\n",
      "epoch: 0, iteration: 4400, batch_loss: 3.309, batch_accuracy: 0.371\n",
      "epoch: 1, iteration: 200, batch_loss: 4.884, batch_accuracy: 0.528\n",
      "epoch: 1, iteration: 400, batch_loss: 3.594, batch_accuracy: 0.303\n",
      "epoch: 1, iteration: 600, batch_loss: 3.622, batch_accuracy: 0.335\n",
      "epoch: 1, iteration: 800, batch_loss: 3.255, batch_accuracy: 0.335\n",
      "epoch: 1, iteration: 1000, batch_loss: 3.217, batch_accuracy: 0.354\n",
      "epoch: 1, iteration: 1200, batch_loss: 3.479, batch_accuracy: 0.320\n",
      "epoch: 1, iteration: 1400, batch_loss: 3.121, batch_accuracy: 0.390\n",
      "epoch: 1, iteration: 1600, batch_loss: 3.811, batch_accuracy: 0.287\n",
      "epoch: 1, iteration: 1800, batch_loss: 3.378, batch_accuracy: 0.285\n",
      "epoch: 1, iteration: 2000, batch_loss: 3.431, batch_accuracy: 0.311\n",
      "epoch: 1, iteration: 2200, batch_loss: 3.392, batch_accuracy: 0.362\n",
      "epoch: 1, iteration: 2400, batch_loss: 3.433, batch_accuracy: 0.320\n",
      "epoch: 1, iteration: 2600, batch_loss: 3.165, batch_accuracy: 0.329\n",
      "epoch: 1, iteration: 2800, batch_loss: 3.103, batch_accuracy: 0.319\n",
      "epoch: 1, iteration: 3000, batch_loss: 3.176, batch_accuracy: 0.347\n",
      "epoch: 1, iteration: 3200, batch_loss: 2.842, batch_accuracy: 0.416\n",
      "epoch: 1, iteration: 3400, batch_loss: 3.186, batch_accuracy: 0.333\n",
      "epoch: 1, iteration: 3600, batch_loss: 3.237, batch_accuracy: 0.360\n",
      "epoch: 1, iteration: 3800, batch_loss: 3.113, batch_accuracy: 0.385\n",
      "epoch: 1, iteration: 4000, batch_loss: 3.022, batch_accuracy: 0.450\n",
      "epoch: 1, iteration: 4200, batch_loss: 2.828, batch_accuracy: 0.375\n",
      "epoch: 1, iteration: 4400, batch_loss: 3.045, batch_accuracy: 0.401\n",
      "epoch: 2, iteration: 200, batch_loss: 4.095, batch_accuracy: 0.623\n",
      "epoch: 2, iteration: 400, batch_loss: 2.914, batch_accuracy: 0.394\n",
      "epoch: 2, iteration: 600, batch_loss: 3.029, batch_accuracy: 0.446\n",
      "epoch: 2, iteration: 800, batch_loss: 2.692, batch_accuracy: 0.410\n",
      "epoch: 2, iteration: 1000, batch_loss: 2.743, batch_accuracy: 0.442\n",
      "epoch: 2, iteration: 1200, batch_loss: 2.997, batch_accuracy: 0.410\n",
      "epoch: 2, iteration: 1400, batch_loss: 2.833, batch_accuracy: 0.417\n",
      "epoch: 2, iteration: 1600, batch_loss: 3.339, batch_accuracy: 0.325\n",
      "epoch: 2, iteration: 1800, batch_loss: 2.698, batch_accuracy: 0.403\n",
      "epoch: 2, iteration: 2000, batch_loss: 2.991, batch_accuracy: 0.377\n",
      "epoch: 2, iteration: 2200, batch_loss: 3.045, batch_accuracy: 0.404\n",
      "epoch: 2, iteration: 2400, batch_loss: 3.090, batch_accuracy: 0.378\n",
      "epoch: 2, iteration: 2600, batch_loss: 2.823, batch_accuracy: 0.383\n",
      "epoch: 2, iteration: 2800, batch_loss: 2.699, batch_accuracy: 0.379\n",
      "epoch: 2, iteration: 3000, batch_loss: 2.820, batch_accuracy: 0.415\n",
      "epoch: 2, iteration: 3200, batch_loss: 2.211, batch_accuracy: 0.539\n",
      "epoch: 2, iteration: 3400, batch_loss: 2.595, batch_accuracy: 0.412\n",
      "epoch: 2, iteration: 3600, batch_loss: 2.951, batch_accuracy: 0.395\n",
      "epoch: 2, iteration: 3800, batch_loss: 2.480, batch_accuracy: 0.492\n",
      "epoch: 2, iteration: 4000, batch_loss: 2.714, batch_accuracy: 0.466\n",
      "epoch: 2, iteration: 4200, batch_loss: 2.516, batch_accuracy: 0.415\n",
      "epoch: 2, iteration: 4400, batch_loss: 2.782, batch_accuracy: 0.425\n",
      "epoch: 3, iteration: 200, batch_loss: 3.346, batch_accuracy: 0.723\n",
      "epoch: 3, iteration: 400, batch_loss: 2.281, batch_accuracy: 0.495\n",
      "epoch: 3, iteration: 600, batch_loss: 2.553, batch_accuracy: 0.497\n",
      "epoch: 3, iteration: 800, batch_loss: 2.253, batch_accuracy: 0.486\n",
      "epoch: 3, iteration: 1000, batch_loss: 2.375, batch_accuracy: 0.501\n",
      "epoch: 3, iteration: 1200, batch_loss: 2.605, batch_accuracy: 0.485\n",
      "epoch: 3, iteration: 1400, batch_loss: 2.554, batch_accuracy: 0.442\n",
      "epoch: 3, iteration: 1600, batch_loss: 2.893, batch_accuracy: 0.415\n",
      "epoch: 3, iteration: 1800, batch_loss: 2.168, batch_accuracy: 0.529\n",
      "epoch: 3, iteration: 2000, batch_loss: 2.646, batch_accuracy: 0.426\n",
      "epoch: 3, iteration: 2200, batch_loss: 2.789, batch_accuracy: 0.420\n",
      "epoch: 3, iteration: 2400, batch_loss: 2.752, batch_accuracy: 0.422\n",
      "epoch: 3, iteration: 2600, batch_loss: 2.552, batch_accuracy: 0.419\n",
      "epoch: 3, iteration: 2800, batch_loss: 2.398, batch_accuracy: 0.434\n",
      "epoch: 3, iteration: 3000, batch_loss: 2.570, batch_accuracy: 0.423\n",
      "epoch: 3, iteration: 3200, batch_loss: 1.778, batch_accuracy: 0.594\n",
      "epoch: 3, iteration: 3400, batch_loss: 2.193, batch_accuracy: 0.479\n",
      "epoch: 3, iteration: 3600, batch_loss: 2.689, batch_accuracy: 0.424\n",
      "epoch: 3, iteration: 3800, batch_loss: 2.001, batch_accuracy: 0.575\n",
      "epoch: 3, iteration: 4000, batch_loss: 2.494, batch_accuracy: 0.494\n",
      "epoch: 3, iteration: 4200, batch_loss: 2.328, batch_accuracy: 0.450\n",
      "epoch: 3, iteration: 4400, batch_loss: 2.573, batch_accuracy: 0.462\n",
      "epoch: 4, iteration: 200, batch_loss: 2.882, batch_accuracy: 0.806\n",
      "epoch: 4, iteration: 400, batch_loss: 1.905, batch_accuracy: 0.582\n",
      "epoch: 4, iteration: 600, batch_loss: 2.198, batch_accuracy: 0.534\n",
      "epoch: 4, iteration: 800, batch_loss: 2.012, batch_accuracy: 0.521\n",
      "epoch: 4, iteration: 1000, batch_loss: 2.161, batch_accuracy: 0.527\n",
      "epoch: 4, iteration: 1200, batch_loss: 2.353, batch_accuracy: 0.504\n",
      "epoch: 4, iteration: 1400, batch_loss: 2.378, batch_accuracy: 0.453\n",
      "epoch: 4, iteration: 1600, batch_loss: 2.556, batch_accuracy: 0.455\n",
      "epoch: 4, iteration: 1800, batch_loss: 1.881, batch_accuracy: 0.581\n",
      "epoch: 4, iteration: 2000, batch_loss: 2.382, batch_accuracy: 0.453\n",
      "epoch: 4, iteration: 2200, batch_loss: 2.626, batch_accuracy: 0.431\n",
      "epoch: 4, iteration: 2400, batch_loss: 2.494, batch_accuracy: 0.449\n",
      "epoch: 4, iteration: 2600, batch_loss: 2.372, batch_accuracy: 0.454\n",
      "epoch: 4, iteration: 2800, batch_loss: 2.145, batch_accuracy: 0.484\n",
      "epoch: 4, iteration: 3000, batch_loss: 2.382, batch_accuracy: 0.443\n",
      "epoch: 4, iteration: 3200, batch_loss: 1.514, batch_accuracy: 0.646\n",
      "epoch: 4, iteration: 3400, batch_loss: 1.971, batch_accuracy: 0.525\n",
      "epoch: 4, iteration: 3600, batch_loss: 2.501, batch_accuracy: 0.428\n",
      "epoch: 4, iteration: 3800, batch_loss: 1.739, batch_accuracy: 0.622\n",
      "epoch: 4, iteration: 4000, batch_loss: 2.302, batch_accuracy: 0.519\n",
      "epoch: 4, iteration: 4200, batch_loss: 2.164, batch_accuracy: 0.486\n",
      "epoch: 4, iteration: 4400, batch_loss: 2.392, batch_accuracy: 0.487\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "max_epoch = 5\n",
    "batch_loss = 0\n",
    "batch_accuracy = 0\n",
    "batch_size = 200\n",
    "for epoch in xrange(max_epoch):\n",
    "    for i in xrange(len(train_label_inputs)):\n",
    "        feed = {\n",
    "            image_features: np.reshape(train_image_features[i], [-1, 4096]),\n",
    "            label_input:train_label_inputs[i], \n",
    "            target:train_label_targets[i],\n",
    "            dropout:0.5, \n",
    "        }\n",
    "        loss, tmp_accuracy, _ = sess.run([cross_entropy, accuracy, optimizer], feed_dict=feed)\n",
    "        batch_loss += loss\n",
    "        batch_accuracy += tmp_accuracy\n",
    "        if i % batch_size == 0 and i != 0:\n",
    "            print \"epoch: %d, iteration: %d, batch_loss: %.3f, batch_accuracy: %.3f\" % (\n",
    "                epoch, i, batch_loss / batch_size, batch_accuracy / batch_size)\n",
    "            batch_loss = 0\n",
    "            batch_accuracy = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test per-sample accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracy: 0.394\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = 0\n",
    "for i in xrange(len(test_label_inputs)):\n",
    "    feed = {\n",
    "        image_features: np.reshape(test_image_features[i], [-1, 4096]),\n",
    "        label_input:test_label_inputs[i], \n",
    "        target:test_label_targets[i],\n",
    "        dropout:1.0, \n",
    "    }\n",
    "    tmp_accuracy = sess.run(accuracy, feed_dict=feed)\n",
    "    test_accuracy += tmp_accuracy\n",
    "print \"test_accuracy: %.3f\" % (test_accuracy / len(test_label_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
